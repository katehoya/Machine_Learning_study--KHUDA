![image](https://github.com/user-attachments/assets/18a7b43f-bd9f-4d3d-bb2e-a39b84ed2819)# 8. Dimensionality Reduction  

### 8.2.1 투영  
대부분의 실전 문제의 training sample은 고차원 공간 안의 저차원 부분공간에 놓여있다.  
해당 부분 공간에 투영하는 것.  
![image](https://github.com/user-attachments/assets/8c724b07-3d59-47f0-89ec-bc8194ff263a)  
![image](https://github.com/user-attachments/assets/cd238757-6cc3-45c0-80ea-dcd874e10770)  

스위스롤 데이터 셋
![image](https://github.com/user-attachments/assets/65a5fe0e-cdef-4cc8-a958-91bd4f17e29b)   
![image](https://github.com/user-attachments/assets/187910e9-2845-4237-a0bc-97ac433da610)  

### 8.2.2 2D 매니폴드  
: 고차원 공간에서 휘어진 2D 모양.  
그러나 항상 편리한 것은 아니다.
매니폴드 가정이란 저차원의 매니폴드 공간에 표현되면 더 간단해 진다는 가정이다.  
![image](https://github.com/user-attachments/assets/f7f3f477-1fda-4ec6-8d7d-e6ada8ef2d0e)  

## 8.3 주성분 분석(PCA)  
 - 먼저 데이터에 가장 가까운 초평면을 정의하고,  
 - 데이터를 이 평면에 투영시킨다  
 - 분산이 최대한 보존되도록.  
 -> 원본 데이터셋과 투영된 데이터셋 간의 평균제곱거리가 가장 작은 것.
   ![image](https://github.com/user-attachments/assets/edbb2f34-0110-449c-9349-c1a186911cab)

### 8.3.2 주성분  
주성분 : 분산이 최대인 축, 그 축에 수직이면서 남은 분산을 최대한 보존하는 축, 이전 두 축에 수직이면서 .....  
주성분 찾는 법 : 특잇값 분해(SVD)  
주성분 개수 : 충분한 분산이 될 때까지 늘림.  
![image](https://github.com/user-attachments/assets/9b3e48d9-b986-43b7-bf4b-e3d0358092c9)  
재구성 오차 : 원본 데이터와, pca 변환 후 원상복구한 것 사이의 MSE  
![image](https://github.com/user-attachments/assets/618944c9-ffcc-4eb4-a9ad-7686b41c0935)  
IPCA : 기존 PCA와 달리 전체 training set을 메모리에 올리지 않고, 미니배치로 나눈 뒤 순차적으로 하나씩 넣음  

## 지역 선형 임베딩  
 : 비선형 차원축소 기술로, 투영에 의존하지 않음.  
 ![image](https://github.com/user-attachments/assets/e11b3131-6692-4b59-b9e0-8310e18bf491)  


# 9. 비지도 학습  
![image](https://github.com/user-attachments/assets/5d189356-1a1f-4648-8870-1c682b951b33)  
고객 분류, 이미지 분할, 검색엔진 등에서 쓰임  

### 9.1.1 K 평균  
![image](https://github.com/user-attachments/assets/f5db1d2f-6ce3-45c6-a6b0-fd24e684007f)   
- sample과 센터로이드간의 거리로 분류하기 때문에 부정확한 sample 분류가 발생함.  
![image](https://github.com/user-attachments/assets/d4e86929-4404-4b53-8be6-99bb64d37fd2)
이를 해결하기 위해 샘플을 하나의 군집에 할당하는 하드 군집보다 클러스터마다 샘플에 점수를 부여하는 소프트 군집을 쓰기도 함.
일고리즘
1. 센터로이드를 랜덤하게 설정(샘플을 랜덤하게 뽑아서)  
2. 샘플을 센터로이드에 할당함.  
3. 센터로이드가 변하지 않을 때까지 이 과정을 반복
일반적으로 꽤 적은 횟수 안에 최적의 클러스터에 도달함.
아래 그림도 세 번만에 도달.
![image](https://github.com/user-attachments/assets/c590618b-78b4-49cf-aa2d-d496658296f1)
그러나 local minima에 빠질 수 있는 위험이 있다.
![image](https://github.com/user-attachments/assets/56c3d0b5-c5e9-4106-9986-8d1c912c8d51)
최적의 클러스터임을 판별하는 방법 :   
inertia. : 각 샘플과 가장 가까운 센터로이드 사이의 MSE.
inertia가 작을 수록 좋다.  
![image](https://github.com/user-attachments/assets/c6569de5-baf7-4341-ab47-fcd38f03df0e)

적절한 k 개수를 찾는 방법 :   
inertia(x) : k가 커질수록 inertia는 작아지기 때문에 적절한 개수의 k를 정하기에는 좋은 지표가 아니다.  
![image](https://github.com/user-attachments/assets/2a6253ee-7a32-480f-b5c7-b72f4eab55a6)

실루엣 점수 : 모든 샘플에 대한 실루엣 계수의 평균.  
실루엣 계수 : (b-a) / max(a,b)  
a : 동일한 클러스터에 있는 다른 샘플까지의 평균거리.  
b : 가장 가까운 클러스터까지의 평균거리.  
![image](https://github.com/user-attachments/assets/1d82d330-8721-4df5-9d28-21d9a93785a3)   
![image](https://github.com/user-attachments/assets/4e5f35e4-378c-4c4b-b1e8-30c4328f8dc2)   

### 9.1.2 k-평균의 한계  
클러스터의 밀집도가 다르거나 원형이 아닌 경우 질 작동하지 않는다.  
![image](https://github.com/user-attachments/assets/a412526d-3743-463f-ba69-dd2596f77f3b)   

### 9.1.3 군집을 이용한 이미지 분할  
색상 분할, 시맨틱 분할, 인스턴스 분할.  
### 9.1.4 군집을 이용한 준지도 학습.  
레이블이 없는 데이터가 레이블 있는 데이터보다 많을 때.  

### 9.1.5 DBSCAN  
1. 각 샘플에서 특정 거리 내에 샘플이 몇 개 있는지 확인.  
2. 특정 개수 이상 있다면 핵심 샘플로 간주. 즉 핵심 샘플은 밀집된 지역에 있는 샘플이다.  
3. 핵심 샘플의 이웃에 있는 샘플은 같은 클러스터이다.  
4. 핵심 샘플도 아니고 이웃도 아닌 샘플은 이상치이다.
![image](https://github.com/user-attachments/assets/ed5856d5-8edf-4328-b825-8d4a55924022)
![image](https://github.com/user-attachments/assets/dc878092-a2e4-4e89-9fd6-942e211c8f83)
클러스터의 모양과 개수에 상관없이 감지할 수 있는 능력이 있다.
이상치에 안정적이고 하이퍼파라미터가 2개밖에 없다.
그러나 계산 복잡도가 높다.

## 9.2 가우스 혼합   
샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델.  
기댓값 - 최대화 알고리즘  
기댓값 단계 : 샘플을 클러스터에 할당  
최대화 단계 : 클러스터 없데이트   
![image](https://github.com/user-attachments/assets/a7206964-d396-4502-ad9e-707f894f5ddc)  
![image](https://github.com/user-attachments/assets/ff5bffd5-ca6b-47db-accc-e91f34e24ca7)  
![image](https://github.com/user-attachments/assets/7ea99fca-6bd1-499a-94d2-96c552caa558)  
클러스터 개수 : 
이너셔나 실루엣 점수를 이용할 수 없음.  
클러스터가 타원형이거나 크기가 다를 때 이용하기 어렵기 때문  
그래서 이론적 정보 기준을 최소화하는 모델을 사용한다.  
![image](https://github.com/user-attachments/assets/ca5e043a-2706-4bb5-9205-391b6facbe05)  
![image](https://github.com/user-attachments/assets/ddd7a084-88c1-402b-a6fd-8cdb41877920)  

***
# Deep Dive  
- 차원 축소 기법에는 PCA 외에도 여러 기법(Isomap, LDA 등)이 존재합니다. 기법마다 어떤 장단점이 있고, 어떤 경우에 적합할까요?
  - PCA :
    장점 : 계산이 빠르고 직관적임. 데이터의 분산을 최대한 유지하면서 차원을 축소.
    단점 : 선형 변환만 적용되므로 비선형 구조를 가진 데이터에서는 정보 손실이 클 수 있 음.
    적합한 경우 : 고차원 데디터에서 분산이 주요한 특징인 경우, 노이즈 제거 및 시각화를 원할 때.
   - LDA (문서의 집합에서 토픽을 찾아내는 프로세스):
     클래스 간 분산은 최대한 크게, 클래스 내부의 분산은 최대한 작게.
     ![image](https://github.com/user-attachments/assets/80fab860-4bec-4bdf-a53e-313d2f1f6097)  
     장점 : 클래스 간 분산을 최대화하여 분류 성능을 높이는 방향으로 차원 축소.
     단점 : 데이터가 정규 분포를 따른다는 가정이 필요. 클래스 간 분포가 선형적으로 분리되지 않으면 효과가 떨어짐.
     적합한 경우 : 지도 학습에서 분류 문제를 할 떄, 데이터가 정규 분포를 어느 정도 따를 때.
    - Isomap :
      다차원 스케일링 + 주성분 분석  
      장점 : 거리 기반 차 축소 기법으로, 데이터의 비선형적인 구조를 보존할 수 있음.
      단점 : 샘플 개수가 많아지면 계산량이 커지며, 이상치에 민감.
      적합한 경우 : 데이터가 비선형적인 다양체 구조를 가질 때.
    - t-SNE :
      높은 차원에서 비슷한 데이터구조는 낮은 차원에서 가깝게 대응하며,
      비슷하지 않은 데이터구조는 멀리 떨어져 대응된다.
      유사도 : 포인트 A를 중심으로 한 정규 분포에서 확률 밀도에 비례하여 이웃을 ㅅ ㅓㄴ택하면 A가 B를 이웃으로 선택한다는 조건부 확률.
      
      장점 : 데이터의 local structure를 잘 보존하며, 시각화에 유리.
      단점 : 계산 비용이 높고, 새로운 데이터에 적용하기 어려우며, 차원을 너무 많이 줄이면 정보 손실이 클 수 있음.
      적합한 경우 : 데이터 시각화를 위한 경우. (특히 2D or 3D)  
- 비선형성을 가진 분류 문제에서 차원 축소를 할 때 PCA를 사용할 수 있을까요? 더 나은 방법이 있다면, 어떤 방법으로 차원 축소를 할 수 있을까요?
  PCA는 분산을 최대한 유지하는 방향으로 주성분을 찾는 기법이다. 그래서 비선형성을 가진 분류 문제는 분산을 최대화하는 것이 데이터를 잘 표현하는 게 아닐 수도 있다.  EX) 원.  
  커널 PCA나 Isomap등의 방법을 사용.
  KPCA : 함수를 이용해 비선형 매핑을 통해 고차원 공간으로 변환하고, 표준 PCA로 샘플이 구분될 수 있는 저차원으로 데이터를 투영시키는 것.
  단점 : 비싸다.
  이 때 커널 함수를 사용하면 데이터를 직접 변화하지 않고도 고차원 변환 효과를 낼 수 있어서 사용함.  
- 특정 모델들은 차원 축소의 영향을 크게 받기도 하며, 어떤 모델들은 차원 축소 없이 고차원에서도 성능이 좋게 나오기도 합니다. 차원 축소의 영향을 다르게 받는 모델들의 차이점은 무엇일까요?
  차원 축소의 영향을 받는 모델 : KNN, 선형 회귀, DT
  고차원에서 성능이 낮아지는 모델들. 거리 기반, 단순한 가중치 모델. 입력 데이터의 특성이 중요하기 때문에 차원 축소로 인해 특정한 정보가 사라지면 성능이 크게 변함.
  KNN - 이웃 간 유클리드 거리.
  회귀 - feature의 가중치 
  차원 축소 영향 안 받는 모델 : RF, SVM, 딥러닝 모델.
  RF - 불필요한 특성이 많아도 여러 개의 트리를 조합하기 때문에 어느 정도 무시할 수 있음.
  SVM - 고차원에서도 커널 트릭을 사용하여 데이터를 적절한 공간으로 변형.
  딥러닝 - NN은 특징을 자동으로 추출하기 떄문에 원본 차원이 크더라도 중요한 특징만 선택적으로 학습 가능.  
  
 




