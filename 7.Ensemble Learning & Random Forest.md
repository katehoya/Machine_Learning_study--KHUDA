# 7.Ensemble Learning & Random Forest  
## 7.1 투표 기반 분류기  
직접 투표 분류기  
![image](https://github.com/user-attachments/assets/e918f596-baff-4e92-9159-0d7182506b2f)  
 - 각 분류기가 약한 분류기라 할지라도 이 방법의 개별 분류기 중 가장 뛰어난 것보다 정확도가 높은 경우가 많다.
![image](https://github.com/user-attachments/assets/a3adc128-f179-45a2-b9b1-b3c26fe07bb1)

 - 앙상블 기법은 예측기가 가능한 서로 독립적일 때 최고의 성능을 발휘한다.

## 7.2 Bagging & Pasting
방법 1. 분류기들을 서로 다른 알고리즘으로 학습시켜야 한다.
방법 2. 같은 알고리즘을 사용하고 training set의 subset을 랜덤하게 구성하여 분류기를 다르게 학습시킨다.
  bagging : subset에서 중복허용해서 sampling  
  pasting : subset에서 중복허용하지 않고 sampling  
![image](https://github.com/user-attachments/assets/7407bcb6-b560-446e-bd5d-67460df3d750)
모든 예측기의 학습 후에 여러 예측기의 예측을 모아서 새로운 sample에 대한 예측을 만든다.
분류 : 최빈값, 회귀 : 평균.

### 7.2.2 OOB 평가  
중복 허용해서 계속 뽑다보면 평균적으로 각 예측기에 training sample의 63%정도만 선택된다.  
OOB 샘플 : 나머지 37%.  
 --> 사용되지 않는 OOB 샘플을 test set으로 사용  

## 7.4 Random Forest  
랜덤 포레스트 : bagging(or pasting)을 적용한 decision tree의 앙상블.  
Extreme 랜덤 트리 : 가지를 뻗을 때 특성 뿐 아니라 특성의 임계값도 랜덤하게 나누는 것.  
 - 트리 간 상관성을 낮춰서 예측력을 높이고자 한 것.
 - bias는 커지지만 분산은 낮아진다.

## 7.5 Boosting  
 : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법.  
 앞의 모델을 점차 보완해 나가면서 예측기 학습시키는 것.  

### 7.5.1 AdaBoost  
: 이전 모델이 제대로 학습하지 못한 sample의 가중치를 높이는 것.  
![image](https://github.com/user-attachments/assets/af75b97d-fab2-4f47-b884-96388a9d7e58)  
![image](https://github.com/user-attachments/assets/9c7a60f2-10ae-4e25-b91a-6a9b19102541)  
경사하강법과 비슷한 면이 있다.  
경사하강법 : 비용 함수를 최소화하기 위해 파라미터를 조정.  
AdaBoost : 예측력이 좋아지도록 앙상블에 보완된 예측기를 추가.  
단점 : 예측기가 순차적으로 추가되기 때문에 병렬처리를 할 수 없다.  
![image](https://github.com/user-attachments/assets/47223eaf-2420-4911-855d-146119670668)  
![image](https://github.com/user-attachments/assets/41557ae1-d126-4481-9566-1e7d42eb681e)  
![image](https://github.com/user-attachments/assets/53d6842e-01ba-4f38-af98-9889d6d5fb2a)  
![image](https://github.com/user-attachments/assets/43d1fe1d-f3a8-43ed-8893-a0f852cb3b0a)  

### 7.5.2 Gradient Boosting  
AdaBoost랑 비슷하지만 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새 학습기를 학습시킴  
![image](https://github.com/user-attachments/assets/cea39137-8b46-4866-9492-08bc0daa98eb)  
![image](https://github.com/user-attachments/assets/62c3fc2e-b073-49f8-b285-41c515f9ef67)  

### 7.5.3 Histogram-based Gradient Boosting  

## 7.6 Stacking (Stacked Generalization)  
 : 앙상블의 여러 예측기들의 예측을 취합하는 방법을 단순한 함수가 아닌 모델(블렌더 or 메타 학습기)를 또 사용하는 것.  
 ![image](https://github.com/user-attachments/assets/ef62127d-5ee1-4ad5-a711-aa0d30a7a343)  
 ![image](https://github.com/user-attachments/assets/2c01d95f-512f-437b-8260-7352d4119ae9)    

***

## Deep Dive  
 - Split Gain Importance

 - Permutation Feature Importance
 




