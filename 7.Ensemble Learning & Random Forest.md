# 7.Ensemble Learning & Random Forest  
## 7.1 투표 기반 분류기  
직접 투표 분류기  
![image](https://github.com/user-attachments/assets/e918f596-baff-4e92-9159-0d7182506b2f)  
 - 각 분류기가 약한 분류기라 할지라도 이 방법의 개별 분류기 중 가장 뛰어난 것보다 정확도가 높은 경우가 많다.
![image](https://github.com/user-attachments/assets/a3adc128-f179-45a2-b9b1-b3c26fe07bb1)

 - 앙상블 기법은 예측기가 가능한 서로 독립적일 때 최고의 성능을 발휘한다.

## 7.2 Bagging & Pasting
방법 1. 분류기들을 서로 다른 알고리즘으로 학습시켜야 한다.
방법 2. 같은 알고리즘을 사용하고 training set의 subset을 랜덤하게 구성하여 분류기를 다르게 학습시킨다.
  bagging : subset에서 중복허용해서 sampling  
  pasting : subset에서 중복허용하지 않고 sampling  
![image](https://github.com/user-attachments/assets/7407bcb6-b560-446e-bd5d-67460df3d750)
모든 예측기의 학습 후에 여러 예측기의 예측을 모아서 새로운 sample에 대한 예측을 만든다.
분류 : 최빈값, 회귀 : 평균.

### 7.2.2 OOB 평가  
중복 허용해서 계속 뽑다보면 평균적으로 각 예측기에 training sample의 63%정도만 선택된다.  
OOB 샘플 : 나머지 37%.  
 --> 사용되지 않는 OOB 샘플을 test set으로 사용  

## 7.4 Random Forest  
랜덤 포레스트 : bagging(or pasting)을 적용한 decision tree의 앙상블.  
Extreme 랜덤 트리 : 가지를 뻗을 때 특성 뿐 아니라 특성의 임계값도 랜덤하게 나누는 것.  
 - 트리 간 상관성을 낮춰서 예측력을 높이고자 한 것.
 - bias는 커지지만 분산은 낮아진다.

## 7.5 Boosting  
 : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법.  
 앞의 모델을 점차 보완해 나가면서 예측기 학습시키는 것.  

### 7.5.1 AdaBoost  
: 이전 모델이 제대로 학습하지 못한 sample의 가중치를 높이는 것.  
![image](https://github.com/user-attachments/assets/af75b97d-fab2-4f47-b884-96388a9d7e58)  
![image](https://github.com/user-attachments/assets/9c7a60f2-10ae-4e25-b91a-6a9b19102541)  
경사하강법과 비슷한 면이 있다.  
경사하강법 : 비용 함수를 최소화하기 위해 파라미터를 조정.  
AdaBoost : 예측력이 좋아지도록 앙상블에 보완된 예측기를 추가.  
단점 : 예측기가 순차적으로 추가되기 때문에 병렬처리를 할 수 없다.  
![image](https://github.com/user-attachments/assets/47223eaf-2420-4911-855d-146119670668)  
![image](https://github.com/user-attachments/assets/41557ae1-d126-4481-9566-1e7d42eb681e)  
![image](https://github.com/user-attachments/assets/53d6842e-01ba-4f38-af98-9889d6d5fb2a)  
![image](https://github.com/user-attachments/assets/43d1fe1d-f3a8-43ed-8893-a0f852cb3b0a)  

### 7.5.2 Gradient Boosting  
AdaBoost랑 비슷하지만 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새 학습기를 학습시킴  
![image](https://github.com/user-attachments/assets/cea39137-8b46-4866-9492-08bc0daa98eb)  
![image](https://github.com/user-attachments/assets/62c3fc2e-b073-49f8-b285-41c515f9ef67)  

### 7.5.3 Histogram-based Gradient Boosting  

## 7.6 Stacking (Stacked Generalization)  
 : 앙상블의 여러 예측기들의 예측을 취합하는 방법을 단순한 함수가 아닌 모델(블렌더 or 메타 학습기)를 또 사용하는 것.  
 ![image](https://github.com/user-attachments/assets/ef62127d-5ee1-4ad5-a711-aa0d30a7a343)  
 ![image](https://github.com/user-attachments/assets/2c01d95f-512f-437b-8260-7352d4119ae9)    

***

## Deep Dive  
### 1. 트리 모델에서 사용되는 각각의 Feature Importance 기법들의 특징과 장단점은 무엇인지 고민해보세요!  
 - Split Gain Importance
Split : 해당 feature가 모델에서 얼마나 많은 split(분기를 나누는 데)에 영향을 미쳤는지
Gain : 해당 feature가 얻은 총 엔트로피 gain.  

 - Permutation Feature Importance
 
### 2. Tree 계열의 모델들은 다양한 앙상블 기법으로 발전하며 성능을 높여왔습니다. 기본 Tree 모델에서 어떠한 모델들로 발전해왔고, 발전된 모델은 기본 Tree 모델과 어떤 차이를 보이는지, 또는 발전된 모델끼리 어떤 차이를 보이는지 알아봅시다.  
1. Decision Tree
    - 단순한 결정 트리이다.  
    - 엔트로피 / 지니계수 등의 기준으로 노드를 분할한다.  
2. Random Forest
    - Decision Tree 여러 개를 앙상블한 것.
    - 전체의 분산이 줄어든다.
    - 각 트리가 분할 시 feature를 random하게 선택한다.
    - 높은 예측성능, 낮은 속도  
3. Extra Trees
    - Random Forest와 유사하지만 feature를 더욱 random하게 선정.
    - 분할 기준값마저도 최적점이 아닌 random한 값을 선정.
    - RF보다 빠르다.
    - 각 트리 간 더욱 연관성이 없기 때문에 예측 성능 향상.  
4. XGBoost
    - Gradient Boosting + 과적합 방지를 위한 알고리즘.
    - Overfitting 감소
    - 순차적으로 학습하기 때문에 속도가 느림.
5. Light GBM
    - XGBoosting에서 속도를 높인 것.  
    - 트리기준 분할이 아닌 리프기준 분할.  
      트리의 균형을 맞추지 않고 최대 손실 값을 갖는 리프 노드를 지속적으로 분할하면서 깊고 비대칭적인 트리를 생성한다.  
      이렇게 하면 트리 기준 분할 방식에 비해 예측 오류 손실을 최소화할 수 있다.
    - overfitting 발생가능.

### 3. 각각의 Tree 모델들이 어떤 데이터에서 적합한지 비교해봅시다.  
아래의 내용들을 중심으로, 또는 자신이 추가로 조사한 기준에 대하여 비교하고 분석해봅시다. 마지막으로 각각의 Tree가 어떤 상황에서 쓰이는 게 적합할까요?   
모델별로 적합한 데이터 특징 분석 (데이터 크기, 결측치, 범주형 변수 등)  
✅ 모델별로 적합한 데이터 특징 분석 (데이터 크기, 결측치, 범주형 변수 등)  
✅ 데이터 크기가 **작은 경우 vs 큰 경우**에 적합한 트리 모델은 무엇일까?  
 - 데이터 크기가 작은 경우 : Overfitting 위험이 높다. 그래서 bagging 기반 모델이 유리하다.
 - 큰 데이터의 경우 : Boosting이 유리.
 - Bagging은 분산을 줄이는 데 좋고 Boosting은 bias를 줄이는 데 좋다. 
✅ **고차원(Feature가 많은 경우)** vs **저차원(Feature가 적은 경우)**에서 트리 모델 성능 비교
 - Random Forest는 고차원에서 느려질 수 있음.  
 - feature가 적으면 : extra trees, random forest  
 - feature가 많으면 : light gbm, xgboost  
✅ 희소 데이터(Sparse Data, 예: One-hot Encoding)에서 트리 모델 비교
 - boosting 모델이 유리하다



