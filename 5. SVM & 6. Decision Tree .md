## 딥다이브는 맨 아래에 있습니다!  
# 5. Support Vector Machine  

## 5-1. 선형 SVM Classifiation
기본 : Large Margin Classification  
 - 클래스 사이에 폭이 가장 넓은 도로를 찾는 것
![image](https://github.com/user-attachments/assets/93e4c855-6979-4f04-9bd7-2c0c0341b69a)
가장 경계에 걸쳐 있는, 분류의 기준이 되는 샘플 : Support Vector  
Supprt Vector 간의 거리 : Margin  

Hard Margin의 단점 : 이상치에 민감하다. 어떤 예외도 있으면 안되기 때문  
![image](https://github.com/user-attachments/assets/d9130e0e-909a-4bab-bcf0-9fd5c3c73649)  

Soft Margin : 약간의 예외를 허용하는 것.  
C : 규제 Hyperparameter  
![image](https://github.com/user-attachments/assets/79781483-50ea-4944-b92b-245c1c6b400e)  

## 5-2. 비선형 SVM Classification  
 - 비선형 dataset에 특성을 추가해서 선형 분류가 가능하게 하는 것.
![image](https://github.com/user-attachments/assets/2d2240b2-ba21-4750-9245-325b49629406)  
![image](https://github.com/user-attachments/assets/cc01512a-bdb7-4c5a-9700-d1b32a1a0c14)   

###5-2.2 유사도 특성  
유사도 함수 : 각 sample이 랜드마크와 얼마나 닮았는 지 측정함으로써 새로운 좌표축을 설정하는 것.  
![image](https://github.com/user-attachments/assets/9a228d24-33df-4524-886d-f99118497e19)  

## 5-3. SVM 회귀(SVR)  
SVM 분류 : 클래스 간의 도로폭이 최대가 되도록  
SVM 회귀 : 도로 안에 가능한 많은 샘플이 들어가도록  
![image](https://github.com/user-attachments/assets/6b3d7cd8-96e9-4c6c-acb0-6fc1286ee6b9)  

## 5-4. SVM이론  
![image](https://github.com/user-attachments/assets/f6bed6fa-2e0f-488a-8c46-2e186a9d606c)  
선형 SVM 분류기로 예측을 하려면 가중치 벡터 W와 bias b를 찾아야 한다.  
![image](https://github.com/user-attachments/assets/0a249d07-9c04-429b-8fb3-21af0da2abe3)  
가중치 벡터 W에 따라 그래프의 기울기(너비)가 바뀌고, 마진도 바뀌는 걸 알 수 있다.  
![image](https://github.com/user-attachments/assets/17808263-cce2-49d7-996b-0f9e61b74232)  
![image](https://github.com/user-attachments/assets/b29ce02e-795b-43ea-a48d-1fb22a530e46)  
![image](https://github.com/user-attachments/assets/9aa8998b-4134-4764-84cf-2b5206a66954)



# 6. Decision Tree  
![스크린샷 2025-02-04 185148](https://github.com/user-attachments/assets/62061d9b-c898-439a-87fa-e0f94ea7c5f9)  
비교적 간단하다.  
 - 지니 계수  
![image](https://github.com/user-attachments/assets/b51c1bf2-b0f9-4bbb-bf81-6cbefecad464)

 - 엔트로피 불순도
![image](https://github.com/user-attachments/assets/28b369eb-9c85-4cf3-ad35-bdc996bc4d9f)
ex) 모든 sample의 class가 동일하면 entrophy는 0이 된다.


![image](https://github.com/user-attachments/assets/294ef41f-a110-4656-85bc-54ff231ce426)

## 6-3 클래스 확률 추정  
 -- 한 sample이 클라스 k에 속할 확률  
1. 해당 sample의 leaf node 찾기
2. 해당 node에 각 class인 sample의 비율 찾기
3. 그 중 제일 높은 확률에 속한 class

## 6-4.CART 알고리즘  
: 트리를 어떤 기준으로 나눌까? 어떤 특성의 어떤 임계점을 기준으로 나눌까?  
아래 비용함수가 최소가 되는 것으로 나눔  
![image](https://github.com/user-attachments/assets/377a2ed0-24d2-4cfa-b8a2-cf79693a5ff5)  

 - Decision Tree를 탐색하는 시간복잡도 : O(log_2)(n)

### 규제 매개변수 :  
![image](https://github.com/user-attachments/assets/f813a47c-e269-4680-8ba1-ca117f3d36fb)  
![image](https://github.com/user-attachments/assets/8b564a0c-59d5-4595-b50c-d8481f0ac78d)  

## 6-8. Decision Tree를 이용한 회귀  
: 기존의 트리는 class를 예측하는 대신 회귀는 값을 예측한다
![image](https://github.com/user-attachments/assets/b578bf24-428b-47e7-8006-fd4b0789ada9)  
![image](https://github.com/user-attachments/assets/4e38f8fd-e899-4d61-a94a-bb09122bf836)  
![image](https://github.com/user-attachments/assets/6dadb8ab-4df9-47da-b5fb-1161df0ad80f)  
![image](https://github.com/user-attachments/assets/d87a0649-d331-4b61-9640-32f08c585f54)  

## 6-9 축 방향에 대한 민감성  
결정 트리의 경계는 모두 축에 수직입니다.  
그래서 데이터의 방향에 민감합니다.  
![image](https://github.com/user-attachments/assets/970f6ea3-1814-4354-b6fa-9cdc14ff80a7)  


***

## Deep Dive  
### 1. 커널 기법  
 - SVM은 비선형적인 문제에 대해서도 해결책을 제시합니다. 선형적으로 분리되지 않는 데이터를 다룰 때 커널 기법을 활용하여 고차원 공간으로 변환함으로써 분류를 가능하게 합니다. 대표적인 커널에는 선형 커널, 다항식 커널, RBF 커널  등이 있습니다.  
그렇다면  
- 다항식 커널과 RBF 커널의 차이는 무엇인가? 각각의 정확한 개념과  둘 사이의 유사점, 차이점은 무엇인가?   
- 다항식 커널과 RBF 커널의 하이퍼파라미터 각각 d와 γ은 커질수록 오버피팅을 야기할 수 있다. 그 이유는 무엇인가?  
다항식 커널 :  
RBF 커널 :  
차이점 : 다항식 커널은 데이터 간의 내적을 확장하여 비선형 경계를 만든다. RBF 커널은 두 점 간의 차이를 기반으로 경계를 만든다.  
d가 커지면 데이터의 작은 변동에도 크게 반응하게 되니까 오버피팅 가능성이 있다.  
γ가 커지면 점 간의 거리가 조금만 커져도 커널이 작아진다. 그래서 가까이 있는 점들에 민감할 수밖에 없고 이게 심해지면 오버피팅이 된다.  


###2. 가장 좋은 하이퍼파라미터
우리는 SVM을 공부하면서 γ와 C라는 하이퍼파라미터에 대해 공부했습니다.
우리는 머신러닝, 딥러닝을 공부하며 더 많은 하이퍼파라미터들을 마주하게 됩니다.
하이퍼파라미터는 개인의 주관에 따라 적용이 가능하지만, 동시에 모델의 성능에 큰 영향을 끼칩니다.
그렇다면 우리는 어떻게 가장 좋은 하이퍼파라미터를 찾을 수 있을까요? 수많은 숫자를 하나하나 대입해보아야 할까요?
그렇다면  
- 하이퍼파라미터를 튜닝하는 전통적인 방식에는 어떤 방법들이 있으며 이들은 어떤 로직으로 작동할까?  
- AutoML이란 무엇이고, 이것이 항상 전통적인 방식의 하이퍼파라미터 튜닝보다 좋을까?  
- 항상 그렇지 않다면 언제 그런지, 그리고 그 이유는 무엇일까?

전통적인 방법 : 그리드 서치, 랜덤 서치, 베이지안 최적화 등   
AutoML : 하이퍼파라미터를 자동화한 기법으로, 모델 선택과 최적화까지 수행  
전통적인 방식이 더 좋은 경우 :   
도메인 지식이 필요한 경우, resource가 제한적인 경우, AutoML은 ai를 위한 ai기 때문에 데이터가 부족한 경우 좋은 성능이 나오기 어렵
이렇듯 일반적인 성능 최적화를 목표로 하므로 특수한 문제, 데이터인 경우 전문가가 직접 하는 게 더 나을 수 있다.

### 3. 나무를 잘 기르는 법  
결정 트리는 매우 훌륭합니다!  
분류, 회귀문제를 모두 해결할 수 있고, 결측치에 대처가 쉽고 스케일링에서도 상대적으로 자유로운, 좋은 성질을 많이 가진 모델입니다.  
특히나 해석이 쉽게 가능하다는 점과 이 모델의 로직을 생각해보면 단순해 보이기도 합니다.   
하지만 최적의 결정트리를 만들기 위해서는 더 고려할 점들이 있습니다.  
그렇다면  
- 나무의 깊이는 오버피팅과 직결된다. 이를 방지하기 위한 방식 중 프루닝은 무엇이며, 프리 프루닝과 포스트 프루닝의 차이는 무엇인가?  
- 앙상블 기법이 무엇인가? 이 기법이 결정트리에 끼치는 효용과, 그 결과로 볼 수 있는 모델들을 살펴보자.  
- 탐욕 알고리즘은 트리 계열 모델의 핵심이다. 그렇다면 다른 머신러닝 모델들은 탐욕 알고리즘을 사용하지 않을까? 어떤 모델들이 탐욕 알고리즘을 사용하는지, 그 이유와 특징을 살펴보자.  

pre - pruning : 나무 만들기 전에 max_depth, min_samples_split 등의 규제를 정해놓는 것.  
장점 : overfitting 방지  
단점 : 나무를 기르기 전에 정하는 것이기 때문에 미리 정해놓은 규제가 적적하지 않을 수도 있다.(underfitting된다거나, 중요한 정보는 나중의 node에서 나온다거나)  
post - pruning : 나무를 다 키우고 나서 곁가지를 자르는 것  
장점 : 완전히 자란 나무를 보고 정하는 것이기 때문에 일반적으로 pre - pruning보다 성능이 좋다.  
단점 : 나무를 다 키워야 한다는 비용, 계산 문제가 있다  

앙상블 : 여러 개의 결정 트리를 결합하여 성능을 향상  
배깅 - 여러 모델의 결과를 평균 or 투표로 분류  - ex) 랜덤 포레스트   
부스팅 - 순차적으로 학습하면서 이전 모델의 오차를 다음 모델이 보완 - ex) Gradient Boosting  
스태킹 - 서로 다른 모델들의 예측을 기반으로 최종 메타모델을 학습 - ex) XGBoost
탐욕 알고리즘 : 탐욕 알고리즘은 직관적인 구조로 인해 간단하고 빠르지만, 항상 전역 최적 해를 보장하지는 않는다.
