![image](https://github.com/user-attachments/assets/18a7b43f-bd9f-4d3d-bb2e-a39b84ed2819)# 8. Dimensionality Reduction  

### 8.2.1 투영  
대부분의 실전 문제의 training sample은 고차원 공간 안의 저차원 부분공간에 놓여있다.  
해당 부분 공간에 투영하는 것.  
![image](https://github.com/user-attachments/assets/8c724b07-3d59-47f0-89ec-bc8194ff263a)  
![image](https://github.com/user-attachments/assets/cd238757-6cc3-45c0-80ea-dcd874e10770)  

스위스롤 데이터 셋
![image](https://github.com/user-attachments/assets/65a5fe0e-cdef-4cc8-a958-91bd4f17e29b)   
![image](https://github.com/user-attachments/assets/187910e9-2845-4237-a0bc-97ac433da610)  

### 8.2.2 2D 매니폴드  
: 고차원 공간에서 휘어진 2D 모양.  
그러나 항상 편리한 것은 아니다.
매니폴드 가정이란 저차원의 매니폴드 공간에 표현되면 더 간단해 진다는 가정이다.  
![image](https://github.com/user-attachments/assets/f7f3f477-1fda-4ec6-8d7d-e6ada8ef2d0e)  

## 8.3 주성분 분석(PCA)  
 - 먼저 데이터에 가장 가까운 초평면을 정의하고,  
 - 데이터를 이 평면에 투영시킨다  
 - 분산이 최대한 보존되도록.  
 -> 원본 데이터셋과 투영된 데이터셋 간의 평균제곱거리가 가장 작은 것.
   ![image](https://github.com/user-attachments/assets/edbb2f34-0110-449c-9349-c1a186911cab)

### 8.3.2 주성분  
주성분 : 분산이 최대인 축, 그 축에 수직이면서 남은 분산을 최대한 보존하는 축, 이전 두 축에 수직이면서 .....  
주성분 찾는 법 : 특잇값 분해(SVD)  
주성분 개수 : 충분한 분산이 될 때까지 늘림.  
![image](https://github.com/user-attachments/assets/9b3e48d9-b986-43b7-bf4b-e3d0358092c9)  
재구성 오차 : 원본 데이터와, pca 변환 후 원상복구한 것 사이의 MSE  
![image](https://github.com/user-attachments/assets/618944c9-ffcc-4eb4-a9ad-7686b41c0935)  
IPCA : 기존 PCA와 달리 전체 training set을 메모리에 올리지 않고, 미니배치로 나눈 뒤 순차적으로 하나씩 넣음  

## 지역 선형 임베딩  
 : 비선형 차원축소 기술로, 투영에 의존하지 않음.  
 ![image](https://github.com/user-attachments/assets/e11b3131-6692-4b59-b9e0-8310e18bf491)  


# 9. 비지도 학습  
![image](https://github.com/user-attachments/assets/5d189356-1a1f-4648-8870-1c682b951b33)  
고객 분류, 이미지 분할, 검색엔진 등에서 쓰임  

### 9.1.1 K 평균  
![image](https://github.com/user-attachments/assets/f5db1d2f-6ce3-45c6-a6b0-fd24e684007f)  
- sample과 센터로이드간의 거리로 분류하기 때문에 부정확한 sample 분류가 발생함.
![image](https://github.com/user-attachments/assets/d4e86929-4404-4b53-8be6-99bb64d37fd2)

















